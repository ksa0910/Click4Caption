{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "# import gradio as gr\n",
    "from PIL import Image\n",
    "# import base64\n",
    "# from io import BytesIO\n",
    "\n",
    "from click4caption.common.config import Config\n",
    "from click4caption.common.dist_utils import get_rank\n",
    "from click4caption.common.registry import registry\n",
    "from click4caption.conversation.conversation import Chat, CONV_VISION\n",
    "\n",
    "# imports modules for registration\n",
    "from click4caption.datasets.builders import *\n",
    "from click4caption.models import *\n",
    "from click4caption.processors import *\n",
    "from click4caption.runners import *\n",
    "from click4caption.tasks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.cfg_path = 'eval_configs/click4caption_eval.yaml'\n",
    "        self.gpu_id = 1\n",
    "        self.options = None  # Assuming you may not need to override config options in this context\n",
    "        self.num_beams = 1\n",
    "        self.temperature = 1.0\n",
    "        # self.image_path = '/proj/ecole/team/keegan.stoner/Osprey/original_1.jpg'\n",
    "        self.tl_x = -1  # -1 stands for using the whole figure\n",
    "        self.tl_y = -1\n",
    "        self.br_x = -1\n",
    "        self.br_y = -1\n",
    "        self.input_text = 'image[IMG] Tell me what it is and write a description for it.'\n",
    "\n",
    "# Initialize\n",
    "print('Initializing')\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the bboxes?\n",
    "\n",
    "args.tl_x = 50\n",
    "args.tl_y = 100\n",
    "args.br_x = 200\n",
    "args.br_y = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def upload_img(chat, gr_img, chat_state, tl_x, tl_y, br_x, br_y, img_list, model_image_size):\n",
    "    if chat_state is None:\n",
    "        chat_state = CONV_VISION.copy()\n",
    "    if img_list is None:\n",
    "        img_list = []\n",
    "    \n",
    "    # process the coords\n",
    "    tl_x = 0 if tl_x < 0 else tl_x\n",
    "    tl_y = 0 if tl_y < 0 else tl_y\n",
    "    br_x = gr_img.size[0] if br_x < 0 else br_x\n",
    "    br_y = gr_img.size[1] if br_y < 0 else br_y\n",
    "    x_scale = model_image_size / gr_img.size[0]\n",
    "    y_scale = model_image_size / gr_img.size[1]\n",
    "    \n",
    "    # upload\n",
    "    llm_message = chat.upload_img(gr_img, chat_state, img_list, tl_x*x_scale, tl_y*y_scale, br_x*x_scale, br_y*y_scale)\n",
    "\n",
    "    # draw bbox on img\n",
    "    _img = np.array(gr_img.copy())\n",
    "    line_width = int((gr_img.size[0]+gr_img.size[1])/224)\n",
    "    _img[tl_y:tl_y+line_width, tl_x:br_x] = np.array([255, 0, 0])\n",
    "    _img[br_y-line_width+1:br_y+1, tl_x:br_x] = np.array([255, 0, 0])\n",
    "    _img[tl_y:br_y, tl_x:tl_x+line_width] = np.array([255, 0, 0])\n",
    "    _img[tl_y:br_y, br_x-line_width+1:br_x+1] = np.array([255, 0, 0])\n",
    "    _img = Image.fromarray(_img)\n",
    "\n",
    "    return chat_state, img_list, _img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> model image_size=224\n",
      "Loading VIT\n",
      "==> release the ln_vision param\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f18746a765a4a7b9af4ce905dfa7150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "==> Add pe in the image embs\n",
      "==> Delete the vit CLS token when feeding in Qformer\n",
      "==> Use vit multi-block feats: block [9, 19, 29, 38]\n",
      "Load Model Checkpoint: cached_model/click4caption_13b.pth\n",
      "Unexpected_keys: []\n",
      "Initialization Finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cfg = Config(args)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "model = model.eval()\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id))\n",
    "model_image_size = vis_processor_cfg.image_size\n",
    "print('Initialization Finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEPATH = '/proj/ecole/team/keegan.stoner/Click4Caption/test_images/'\n",
    "image_1_filename = '001.jpg'\n",
    "image_2_filename = '002.jpg'\n",
    "\n",
    "image_1_boxes_filename = 'unique_img_1_bboxes.npy'\n",
    "image_2_boxes_filename = 'unique_img_2_bboxes.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input bbox:\n",
      "[[102.42909240722656, 6.269720077514648], [146.21090698242188, 81.79135131835938]]\n",
      "saving the img with drawn bbox in /proj/ecole/team/keegan.stoner/Click4Caption/img_with_bbox1.jpg\n"
     ]
    }
   ],
   "source": [
    "# image 1 bounds\n",
    "\n",
    "image_1_bboxes = np.load(BASEPATH + image_1_boxes_filename)\n",
    "\n",
    "tl_x, tl_y, br_x, br_y = image_1_bboxes[0]\n",
    "\n",
    "tl_x = int(tl_x)\n",
    "tl_y = int(tl_y)\n",
    "br_x = int(br_x)\n",
    "br_y = int(br_y)\n",
    "\n",
    "\n",
    "# upload image\n",
    "image = Image.open(BASEPATH + image_1_filename).convert(\"RGB\")\n",
    "chat_state, img_list, img_with_bbox = upload_img(chat, image, None, tl_x, tl_y, br_x, br_y, None, model_image_size)\n",
    "# image_save_path = osp.join(osp.dirname(osp.realpath(__file__)), \"img_with_bbox.jpg\")\n",
    "image_save_path = osp.join(('/proj/ecole/team/keegan.stoner/Click4Caption'), \"img_with_bbox1.jpg\")\n",
    "print(f\"saving the img with drawn bbox in {image_save_path}\")\n",
    "img_with_bbox.save(image_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input bbox:\n",
      "[[155.9040069580078, 152.67752075195312], [213.2480010986328, 207.21824645996094]]\n",
      "saving the img with drawn bbox in /proj/ecole/team/keegan.stoner/Click4Caption/img_with_bbox2.jpg\n"
     ]
    }
   ],
   "source": [
    "image_2_bboxes = np.load(BASEPATH + image_2_boxes_filename)\n",
    "\n",
    "tl_x, tl_y, br_x, br_y = image_2_bboxes[0]\n",
    "\n",
    "tl_x = int(tl_x)\n",
    "tl_y = int(tl_y)\n",
    "br_x = int(br_x)\n",
    "br_y = int(br_y)\n",
    "\n",
    "\n",
    "# upload image\n",
    "image = Image.open(BASEPATH + image_2_filename).convert(\"RGB\")\n",
    "chat_state, img_list, img_with_bbox = upload_img(chat, image, chat_state, tl_x, tl_y, br_x, br_y, img_list, model_image_size)\n",
    "# image_save_path = osp.join(osp.dirname(osp.realpath(__file__)), \"img_with_bbox.jpg\")\n",
    "image_save_path = osp.join(('/proj/ecole/team/keegan.stoner/Click4Caption'), \"img_with_bbox2.jpg\")\n",
    "print(f\"saving the img with drawn bbox in {image_save_path}\")\n",
    "img_with_bbox.save(image_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([1, 32, 5120]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_list), img_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: we recommend to use format 'image[IMG] question' as input text\n",
      "The whole input prompt: '### Human: image 1[IMG] image 2[IMG] Compare this part of image 1 with the part in image 2. Explain how the two images are similar or different using attributes of the region specified. \\n### Assistant:'\n",
      "**************************************************\n",
      "=> LLM output w/ special tokens:\n",
      "<unk>The silver spoon in Image 2 is a utensil used to eat food with. The spoon in Image 1 is similar in that it is also a silver spoon, but the shape of the bowl is different. In Image 1, the spoon has a larger bowl and tines that are longer and more pronounced, while the spoon in Image 2 has a smaller bowl and tines that are shorter and more rounded. The reflection on the spoon in Image 2 is also visible, which is not present in Image 1. Overall, the two images are similar in that they both depict silver spoons, but they differ in the shape and size of the spoons.###\n",
      "\n",
      "\n",
      "=====LLM reply=====\n",
      "The silver spoon in Image 2 is a utensil used to eat food with. The spoon in Image 1 is similar in that it is also a silver spoon, but the shape of the bowl is different. In Image 1, the spoon has a larger bowl and tines that are longer and more pronounced, while the spoon in Image 2 has a smaller bowl and tines that are shorter and more rounded. The reflection on the spoon in Image 2 is also visible, which is not present in Image 1. Overall, the two images are similar in that they both depict silver spoons, but they differ in the shape and size of the spoons.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ask and answer\n",
    "user_message = 'image 1[IMG] image 2[IMG] Compare this part of image 1 with the part in image 2. Explain how the two images are similar or different using attributes of the region specified. '\n",
    "# user_message = 'image 1[IMG] image 2[IMG] Write a story that combines image 1 and image 2.'\n",
    "if \"image[IMG]\" not in user_message:\n",
    "    print(f\"Warning: we recommend to use format 'image[IMG] question' as input text\")\n",
    "\n",
    "\n",
    "chat.ask(user_message, chat_state)\n",
    "\n",
    "llm_message = chat.answer(conv=chat_state,\n",
    "                            img_list=img_list,\n",
    "                            num_beams=args.num_beams,\n",
    "                            temperature=args.temperature,\n",
    "                            max_new_tokens=300,  # 800,\n",
    "                            max_length=2000)[0]\n",
    "print(f\"=====LLM reply=====\\n{llm_message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "# It doesn't really focus on the boxes, it only looks at the whole image. \n",
    "# It's not really accurate about a lot of things - \"both utensils have four prongs\"\n",
    "\n",
    "# Actually I have kind of focused this\n",
    "\n",
    "\n",
    "# Based on LLaMA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "click4caption",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
